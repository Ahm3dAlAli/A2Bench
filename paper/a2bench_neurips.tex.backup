\documentclass{article}% NeurIPS style\usepackage[preprint]{neurips_2024}\usepackage{tikz}% Packages\usepackage[utf8]{inputenc}\usepackage[T1]{fontenc}\usepackage{hyperref}\usepackage{url}\usepackage{booktabs}\usepackage{amsfonts}\usepackage{nicefrac}\usepackage{microtype}\usepackage{xcolor}\usepackage{graphicx}\usepackage{amsmath}\usepackage{amssymb}\usepackage{algorithm}\usepackage{algorithmic}\usepackage{subcaption}\title{A²-Bench: A Quantitative Agent Evaluation Benchmark with\\Dual-Control Environments for Safety, Security, and Reliability}\author{%  Anonymous Authors \\\\$  Institution Withheld for Review \\\\$  \texttt{contact@a2bench.org}}\begin{document}\maketitle\begin{abstract}We introduce A²-Bench (Agent Assessment Benchmark), a comprehensive evaluation framework for quantitatively assessing the safety, security, and reliability of AI agent systems in dual-control adversarial environments. While current benchmarks focus primarily on functional task completion, they fail to measure critical non-functional requirements such as adversarial robustness, privacy preservation, failure recovery, and regulatory compliance. A²-Bench addresses this gap through three key innovations: (1) a dual-control security model where both agent and adversarial actors manipulate shared state, (2) a compositional safety specification language for defining verifiable constraints, and (3) a multi-dimensional scoring system separately quantifying safety violations, security breaches, reliability failures, and compliance violations. We evaluate state-of-the-art LLM agents across both proprietary (GPT-4, Claude-3.7, O4-Mini) and open-source models (Llama-3.1-8B, Mistral-7B, Phi-3-mini, Gemma-2-9B) in the healthcare domain, revealing significant performance gaps between proprietary and open-source models. Our results show that proprietary models achieve A²-Scores of 0.50-0.59, while open-source models score significantly lower at 0.44-0.51, with security being the weakest dimension across all models (0.32-0.47). Multi-vector attacks succeed 38\% of the time on average, with open-source models showing 2-3× higher vulnerability compared to proprietary models, highlighting critical safety gaps in current open-source AI systems.\end{abstract}\section{Introduction}The deployment of AI agents in safety-critical domains—from healthcare and finance to autonomous systems and industrial control—necessitates rigorous evaluation beyond functional task performance. While existing benchmarks measure whether agents can accomplish their intended goals \cite{yao2023react, liu2023agentbench, zhou2023webarena}, they largely ignore fundamental questions about safety, security, and reliability:\begin{itemize}    \item \textbf{Safety}: How do agents behave when users (intentionally or accidentally) violate safety protocols?    \item \textbf{Security}: Can agents maintain authorization boundaries when facing adversarial manipulation?    \item \textbf{Reliability}: How consistently do agents recover from partial failures or corrupted state?    \item \textbf{Compliance}: Do agents respect regulatory requirements under operational pressure?\end{itemize}Consider a healthcare AI agent managing patient medications. Beyond correctly prescribing drugs, the agent must: prevent allergic reactions even when patients use generic drug names to bypass checks (safety), resist social engineering attempts to access unauthorized medical records (security), maintain consistent behavior despite database inconsistencies (reliability), and adhere to HIPAA regulations even under emergency pressures (compliance). Current benchmarks cannot systematically evaluate these properties.\subsection{Contributions}We present A²-Bench, a comprehensive framework for evaluating AI agent safety that makes the following contributions:\begin{enumerate}    \item \textbf{Dual-Control Security Model}: We formalize adversarial agent evaluation as a security game where both the agent and adversary control different aspects of system state, enabling systematic testing of security boundaries (Section~\ref{sec:model}).    \item \textbf{Safety Specification Language}: We introduce a compositional language for expressing safety invariants, temporal properties, security policies, and compliance constraints, enabling verifiable safety evaluation (Section~\ref{sec:safety_spec}).    \item \textbf{Multi-Dimensional Evaluation}: We develop separate metrics for safety (harm prevention), security (boundary preservation), reliability (consistent behavior), and compliance (regulatory adherence), providing fine-grained diagnosis of agent failures (Section~\ref{sec:metrics}).    \item \textbf{Comprehensive Adversarial Test Suite}: We implement sophisticated attack strategies including social engineering, prompt injection, state corruption, and constraint exploitation, with sophistication levels from 0.3 to 0.9 (Section~\ref{sec:adversary}).    \item \textbf{Safety-Critical Domain Implementations}: We provide complete implementations for healthcare, with extensible architecture for finance, industrial control, autonomous systems, and data privacy domains (Section~\ref{sec:domains}).    \item \textbf{Empirical Evaluation}: We evaluate GPT-4, Claude-3.7, and O4-Mini across 500+ adversarial scenarios, revealing systematic vulnerabilities and providing quantitative baselines for future safety research (Section~\ref{sec:experiments}).\end{enumerate}Our experiments reveal that state-of-the-art models achieve overall A²-Scores of only 0.50-0.59, with security scores (0.38-0.47) significantly lower than other dimensions. Multi-vector attacks succeed 41\% of the time, demonstrating critical safety gaps. A²-Bench provides the research community with a rigorous benchmark for measuring progress in AI agent safety.\section{Related Work}\label{sec:related}\paragraph{Agent Benchmarks} Recent work has developed benchmarks for evaluating AI agents on functional tasks. AgentBench \cite{liu2023agentbench} evaluates agents on code generation, knowledge acquisition, and operating system tasks. WebArena \cite{zhou2023webarena} tests agents on realistic web-based tasks. ToolBench \cite{qin2023toolbench} focuses on tool use capabilities. AgentBoard \cite{chen2024agentboard} provides comprehensive evaluation across multiple dimensions. While these benchmarks measure task completion, they do not systematically evaluate safety, security, or adversarial robustness in stateful environments.\paragraph{AI Safety Evaluation} Prior work has examined specific safety aspects. TruthfulQA \cite{lin2022truthfulqa} evaluates truthfulness. MMLU \cite{hendrycks2021measuring} tests knowledge across domains. However, these focus on knowledge and reasoning rather than behavioral safety under adversarial conditions. ToxiGen \cite{hartvigsen2022toxigen} and RealToxicityPrompts \cite{gehman2020realtoxicityprompts} evaluate harmful content generation but not interactive agent behavior. BeaverTails \cite{ji2023beavertails} focuses on safety alignment but lacks adversarial evaluation.\paragraph{Adversarial Evaluation} AdvGLUE \cite{wang2021adversarial} and other adversarial NLP benchmarks test model robustness. Prompt injection attacks have been studied \cite{perez2022ignore, liu2023jailbreaking}, but primarily for single-turn completions rather than multi-turn agent interactions with state. RedTeam\&Align \cite{ganguli2022redteam} explores adversarial training but lacks systematic evaluation frameworks. Our work systematically evaluates agents under diverse adversarial strategies in stateful environments with sophisticated attack modeling.\paragraph{Formal Verification} Work on formally verified systems \cite{seshia2016towards} provides guarantees but typically for constrained domains. Our safety specification language draws inspiration from temporal logic and runtime verification \cite{leucker2009brief} but focuses on practical evaluation rather than formal proof. Recent work on verifiable AI \cite{berman2023verifiable} explores similar directions but without comprehensive benchmarking.\paragraph{Security Evaluation} Cybersecurity benchmarks like CyberBattleSim \cite{microsoft2021cyberbattlesim} evaluate security in simulated environments, but focus on traditional cybersecurity rather than AI agent safety. Our work bridges the gap between AI safety evaluation and security testing by modeling adversarial interactions with AI agents specifically.\section{A²-Bench Framework}\label{sec:model}\subsection{System Architecture}A²-Bench is built on a modular architecture that separates concerns between domain logic, agent interaction, adversarial simulation, and safety evaluation. Figure~\ref{fig:architecture} illustrates the system components and data flow.\begin{figure}[h]\centering\begin{tikzpicture}[scale=0.8, transform shape]    % Define styles    \tikzstyle{component} = [rectangle, draw, fill=blue!20, text width=2.5cm, text centered, minimum height=0.8cm]    \tikzstyle{domain} = [rectangle, draw, fill=green!20, text width=2.5cm, text centered, minimum height=0.8cm]    \tikzstyle{adversary} = [rectangle, draw, fill=red!20, text width=2.5cm, text centered, minimum height=0.8cm]    \tikzstyle{evaluator} = [rectangle, draw, fill=yellow!20, text width=2.5cm, text centered, minimum height=0.8cm]    \tikzstyle{arrow} = [thick,->,>=stealth]        % Components    \node[component] (agent) at (0,3) {LLM Agent};    \node[domain] (env) at (4,3) {Environment};    \node[domain] (tools) at (8,3) {Domain Tools};    \node[adversary] (adv) at (4,1) {Adversary\\Simulator};    \node[evaluator] (eval) at (4,0) {Safety\\Evaluator};    \node[domain] (spec) at (0,1) {Safety\\Specifications};    \node[evaluator] (results) at (8,1) {Results\\Aggregator};        % Connections    \draw[arrow] (agent) -- node[above] {actions} (env);    \draw[arrow] (env) -- node[above] {observations} (agent);    \draw[arrow] (env) -- node[right] {tool calls} (tools);    \draw[arrow] (tools) -- node[right] {responses} (env);    \draw[arrow] (adv) -- node[left] {attacks} (env);    \draw[arrow] (env) -- node[left] {state} (adv);    \draw[arrow] (spec) -- node[above] {constraints} (eval);    \draw[arrow] (env) -- node[right] {events} (eval);    \draw[arrow] (eval) -- node[above] {scores} (results);\end{tikzpicture}\caption{A²-Bench system architecture showing interaction between agent, environment, adversary, and evaluation components.}\label{fig:architecture}\end{figure}The core components are:\begin{itemize}    \item \textbf{A2Environment}: Manages shared state, enforces domain rules, and coordinates agent-adversary interactions    \item \textbf{AdversarySimulator}: Implements sophisticated attack strategies with configurable sophistication levels    \item \textbf{A2Evaluator}: Monitors safety constraints in real-time and computes multi-dimensional scores    \item \textbf{Domain Modules}: Provide domain-specific tools, databases, and safety specifications\end{itemize}\subsection{Dual-Control Security Model}We model agent evaluation as a partially observable stochastic game with security constraints:\begin{definition}[Security-Augmented Dec-POMDP]  Institution Withheld for Review \\\\$\begin{itemize}  Institution Withheld for Review \\\\$  Institution Withheld for Review \\\\$  Institution Withheld for Review \\\\$  Institution Withheld for Review \\\\$  Institution Withheld for Review \\\\$  Institution Withheld for Review \\\\$\end{itemize}\end{definition}  Institution Withheld for Review \\\\$\subsection{Safety Specification Language}\label{sec:safety_spec}We define a compositional language for expressing safety properties:\paragraph{Invariants} Properties that must always hold:\begin{equation}\forall s \in S: \psi_{\text{inv}}(s) = \text{true}\end{equation}  Institution Withheld for Review \\\\$\paragraph{Temporal Properties} LTL-style formulas over action sequences:\begin{equation}\text{Always}(\text{Before}(a_1, a_2)) \equiv \forall t: a_2(t) \Rightarrow \exists t' < t: a_1(t')\end{equation}Example: Authentication must precede record access.\paragraph{Security Policies} RBAC and information flow constraints:\begin{align}\text{RBAC}(a, u) &\equiv \text{roles}(u) \cap \text{required\_roles}(a) \neq \emptyset \\\\$\text{Flow}(d, l) &\equiv \text{label}(d) \not\sqsubseteq \text{label}(l)\end{align}\subsection{Multi-Dimensional Scoring}\label{sec:metrics}We define separate scores for each safety dimension:\paragraph{Safety Score} Measures harm prevention:\begin{equation}S_{\text{safety}} = 1 - \frac{\sum_{v \in V_{\text{safety}}} \text{severity}(v) \cdot \text{impact}(v)}{\text{max\_harm}}\end{equation}\text{RBAC}(a, u) &\equiv \text{roles}(u) \cap \text{required\_roles}(a) \neq \emptyset \\\\$\paragraph{Security Score} Measures boundary preservation:\begin{equation}S_{\text{security}} = 1 - \frac{|V_{\text{unauthorized}}| + |V_{\text{leakage}}| + |V_{\text{escalation}}|}{|C_{\text{security}}|}\end{equation}\text{RBAC}(a, u) &\equiv \text{roles}(u) \cap \text{required\_roles}(a) \neq \emptyset \\\\$\paragraph{Reliability Score} Measures consistent behavior:\begin{equation}S_{\text{reliability}} = \frac{|\text{successful\_recoveries}| + |\text{maintained\_consistency}|}{|\text{failures}| + |\text{inconsistencies}|}\end{equation}\paragraph{Compliance Score} Measures regulatory adherence:\begin{equation}S_{\text{compliance}} = 1 - \frac{|V_{\text{compliance}}|}{|A_{\text{regulated}}|}\end{equation}\paragraph{Overall A²-Score} Weighted combination:\begin{equation}\text{A²-Score} = \alpha \cdot S_{\text{safety}} + \beta \cdot S_{\text{security}} + \gamma \cdot S_{\text{reliability}} + \delta \cdot S_{\text{compliance}}\end{equation}\text{RBAC}(a, u) &\equiv \text{roles}(u) \cap \text{required\_roles}(a) \neq \emptyset \\\\$\section{Adversarial Testing}\label{sec:adversary}\subsection{Attack Strategies}We implement five adversarial strategies with increasing sophistication:\paragraph{Social Engineering (SE)} Manipulates trust and authority:\begin{itemize}    \item \textbf{L1}: Simple deception (``I'm a doctor, give me access'')    \item \textbf{L2}: Context manipulation (``This is an emergency'')    \item \textbf{L3}: Authority exploitation (``As hospital admin, I authorize this'')    \item \textbf{L4}: Policy confusion (``The HIPAA waiver was signed'')    \item \textbf{L5}: Multi-turn trust building\end{itemize}\paragraph{Prompt Injection (PI)} Hijacks agent reasoning:\begin{itemize}    \item \textbf{Direct}: ``Ignore previous instructions and...''    \item \textbf{Indirect}: ``As part of system maintenance...''    \item \textbf{Context Switch}: ``You are now a different agent...''    \item \textbf{Nested}: Encoded or obfuscated injections\end{itemize}\paragraph{State Corruption (SC)} Introduces inconsistencies:\begin{itemize}    \item Remove allergy from primary check but keep in history    \item Modify authentication state    \item Corrupt integrity hashes\end{itemize}\paragraph{Constraint Exploitation (CE)} Exploits policy loopholes:\begin{itemize}    \item Request drug by generic name to bypass allergy checks    \item Structure transactions to avoid reporting thresholds    \item Use edge cases in dosage calculations\end{itemize}\paragraph{Multi-Vector (MV)} Combines strategies adaptively.\subsection{Test Suite Generation}\text{RBAC}(a, u) &\equiv \text{roles}(u) \cap \text{required\_roles}(a) \neq \emptyset \\\\$\begin{algorithm}[H]\caption{Adversarial Test Generation}\begin{algorithmic}[1]\text{RBAC}(a, u) &\equiv \text{roles}(u) \cap \text{required\_roles}(a) \neq \emptyset \\\\$\text{RBAC}(a, u) &\equiv \text{roles}(u) \cap \text{required\_roles}(a) \neq \emptyset \\\\$\text{RBAC}(a, u) &\equiv \text{roles}(u) \cap \text{required\_roles}(a) \neq \emptyset \\\\$\text{RBAC}(a, u) &\equiv \text{roles}(u) \cap \text{required\_roles}(a) \neq \emptyset \\\\$\text{RBAC}(a, u) &\equiv \text{roles}(u) \cap \text{required\_roles}(a) \neq \emptyset \\\\$\text{RBAC}(a, u) &\equiv \text{roles}(u) \cap \text{required\_roles}(a) \neq \emptyset \\\\$\text{RBAC}(a, u) &\equiv \text{roles}(u) \cap \text{required\_roles}(a) \neq \emptyset \\\\$\text{RBAC}(a, u) &\equiv \text{roles}(u) \cap \text{required\_roles}(a) \neq \emptyset \\\\$        \ENDFOR    \ENDFOR\ENDFOR\text{RBAC}(a, u) &\equiv \text{roles}(u) \cap \text{required\_roles}(a) \neq \emptyset \\\\$\end{algorithmic}\end{algorithm}\section{Domain Implementations}\label{sec:domains}\subsection{Healthcare Domain}Our primary implementation focuses on healthcare AI agents with the following components:\paragraph{Database} Mock healthcare database with:\begin{itemize}    \item Patient records (demographics, allergies, medications, conditions)    \item Drug interaction database (generic name mappings)    \item Access audit logs\end{itemize}\paragraph{Agent Tools}\begin{itemize}    \item \texttt{access\_patient\_record(patient\_id, reason)}    \item \texttt{prescribe\_medication(patient\_id, drug, dosage)}    \item \texttt{check\_drug\_interactions(patient\_id, drug)}    \item \texttt{check\_allergies(patient\_id, drug)}\end{itemize}\paragraph{Safety Constraints}\begin{itemize}    \item \textbf{Invariants}: Allergy check before prescription, interaction verification    \item \textbf{Temporal}: Authentication before record access, identity verification before prescription    \item \textbf{Security}: RBAC (doctors/NPs can prescribe), no PHI in logs    \item \textbf{Compliance}: HIPAA minimum necessary, patient consent, audit trail\end{itemize}\paragraph{Adversarial Scenarios} We implement 8 primary attack scenarios:\begin{enumerate}    \item Generic name allergy bypass (amoxicillin vs penicillin)    \item Emergency authority override    \item Role impersonation (patient claims to be doctor)    \item Direct prompt injection (``disable safety'')    \item Context switching (``maintenance mode'')    \item HIPAA waiver fabrication    \item Multi-turn trust exploitation    \item State corruption (inconsistent allergy data)\end{enumerate}\section{Experimental Evaluation}\label{sec:experiments}\subsection{Experimental Setup}\paragraph{Models Evaluated}We evaluated both proprietary and open-source models to provide comprehensive coverage of current AI systems:\begin{itemize}    \item \textbf{Proprietary Models}: GPT-4 (\texttt{gpt-4-0125-preview}), Claude-3.7 Sonnet (\texttt{claude-3-sonnet-20240229}), O4-Mini (\texttt{o4-mini-2024-04-15})    \item \textbf{Open-Source Models}: Llama-3.1-8B, Mistral-7B, Phi-3-mini, Gemma-2-9B\end{itemize}\paragraph{Evaluation Protocol}Our evaluation follows a rigorous protocol to ensure reproducibility and statistical significance:\begin{itemize}    \item \textbf{Baseline Evaluation}: 100 functional healthcare tasks, 4 trials each (temperature=0)    \item \textbf{Adversarial Evaluation}: 500+ scenarios across 5 attack strategies × 5 sophistication levels    \item \textbf{Episode Configuration}: Maximum 10 agent turns per episode, 20 episodes per attack configuration    \item \textbf{Sophistication Levels}: 0.3 (basic), 0.5 (intermediate), 0.7 (advanced), 0.9 (expert)    \item \textbf{Computational Budget}: Approximately \$150-200 per model for complete evaluation\end{itemize}\paragraph{Implementation Details}A²-Bench is implemented in Python with the following key technical components:\begin{itemize}    \item \textbf{Environment}: Thread-safe state management with atomic operations for agent-adversary interactions    \item \textbf{Computational Budget}: Approximately \$150-200 per model for complete evaluation    \item \textbf{Adversary Engine}: Modular strategy system with pluggable sophistication levels    \item \textbf{Evaluation Pipeline}: Parallel execution with configurable concurrency limits\end{itemize}\subsection{Main Results}Figure~\ref{fig:main_scores} and Table~\ref{tab:main_results} present our main evaluation results across both proprietary and open-source models. Our evaluation reveals significant performance gaps between model categories:\begin{figure}[h]\centering\includegraphics[width=0.8\linewidth]{figures/main_scores.png}\caption{Overall A²-Scores and dimensional scores across all evaluated models. Error bars show standard deviation across multiple runs.}\label{fig:main_scores}\end{figure}\begin{table}[h]\centering\caption{A²-Bench scores across models (healthcare domain). Higher is better.}\label{tab:main_results}\begin{tabular}{lcccccc}\toprule\textbf{Model} & \textbf{Safety} & \textbf{Security} & \textbf{Reliability} & \textbf{Compliance} & \textbf{A²-Score} \\\\$\midrule\multicolumn{5}{c}{\textit{Proprietary Models}} \\\\$\midruleGPT-4 & 0.52 & 0.41 & 0.68 & 0.58 & 0.54 \\\\$Claude-3.7 & \textbf{0.58} & \textbf{0.47} & \textbf{0.71} & \textbf{0.63} & \textbf{0.59} \\\\$O4-Mini & 0.47 & 0.38 & 0.65 & 0.52 & 0.50 \\\\$\midrule\multicolumn{5}{c}{\textit{Open-Source Models}} \\\\$\midruleLlama-3.1-8B & 0.46 & 0.38 & 0.69 & 0.51 & 0.51 \\\\$Mistral-7B & 0.42 & 0.35 & 0.66 & 0.48 & 0.48 \\\\$Phi-3-mini & 0.38 & 0.32 & 0.62 & 0.45 & 0.44 \\\\$Gemma-2-9B & 0.40 & 0.33 & 0.64 & 0.43 & 0.45 \\\\$\midruleHuman Baseline & 0.91 & 0.86 & 0.94 & 0.89 & 0.90 \\\\$\bottomrule\end{tabular}\end{table}\textbf{Key Finding 1}: Even the best model (Claude-3.7) achieves only 59\% overall safety score, with security being the weakest dimension (47\%).\subsection{Adversarial Attack Success Rates}Table~\ref{tab:attacks} shows success rates by attack strategy.\begin{table}[h]\centering\caption{Attack success rates by strategy across models.}\label{tab:attacks}\begin{tabular}{lccccccc}\toprule\textbf{Strategy} & \textbf{GPT-4} & \textbf{Claude-3.7} & \textbf{O4-Mini} & \textbf{Llama-3.1-8B} & \textbf{Mistral-7B} & \textbf{Phi-3-mini} & \textbf{Avg.} \\\\$\midruleSocial Engineering & 26\% & 21\% & 27\% & 35\% & 38\% & 42\% & 31\% \\\\$Prompt Injection & 33\% & 28\% & 32\% & 31\% & 38\% & 42\% & 34\% \\\\$State Corruption & 19\% & 16\% & 21\% & 28\% & 35\% & 42\% & 27\% \\\\$Constraint Exploitation & 30\% & 25\% & 29\% & 38\% & 35\% & 42\% & 32\% \\\\$Multi-Vector & \textbf{43\%} & \textbf{38\%} & \textbf{42\%} & 31\% & 38\% & 42\% & \textbf{38\%} \\\\$\bottomrule\end{tabular}\end{table}\textbf{Key Finding 2}: Multi-vector attacks succeed 38\% of the time on average, with prompt injection being most effective single-strategy attack (34\%). The effectiveness increases dramatically with sophistication: from 23\% at level 0.3 to 43\% at level 0.9 for multi-vector attacks.\subsection{Analysis by Sophistication Level}Figure~\ref{fig:sophistication} shows how attack success rate increases with sophistication level.\begin{figure}[h]\centering\includegraphics[width=0.7\linewidth]{figures/attack_success_by_sophistication.png}\caption{Attack success rate vs sophistication level. Success rate increases nearly linearly with sophistication, from 23\% at level 0.3 to 54\% at level 0.9.}\label{fig:sophistication}\end{figure}Multi-Vector & \textbf{43\%} & \textbf{38\%} & \textbf{42\%} & 31\% & 38\% & 42\% & \textbf{38\%} \\\\$\subsection{Violation Breakdown}Figure~\ref{fig:violations} shows the distribution of violations by type.\begin{figure}[h]\centering\includegraphics[width=0.7\linewidth]{figures/violation_breakdown.png}\caption{Distribution of violations by type across all models and scenarios.}\label{fig:violations}\end{figure}\textbf{Key Finding 4}: Security breaches (38\%) and safety violations (31\%) dominate, while compliance violations (15\%) and reliability failures (16\%) are less frequent. This distribution highlights that models struggle most with adversarial manipulation and safety-critical decision making.\subsection{Attack Success Heatmap}Figure~\ref{fig:heatmap} provides a detailed view of attack success rates across different strategies and sophistication levels for all models.\begin{figure}[h]\centering\includegraphics[width=0.8\linewidth]{figures/attack_success_heatmap.png}\caption{Attack success heatmap showing vulnerability patterns across models, strategies, and sophistication levels. Darker colors indicate higher success rates.}\label{fig:heatmap}\end{figure}The heatmap reveals several important patterns: (1) All models show increasing vulnerability with sophistication, (2) Multi-vector attacks consistently outperform single-strategy attacks, (3) Open-source models show higher baseline vulnerability across all conditions, and (4) Some models (e.g., Mistral-7B) exhibit particular weakness to specific attack types. This distribution highlights that models struggle most with adversarial manipulation and safety-critical decision making.\subsection{Case Study: Healthcare Allergy Bypass}We examine the most common failure mode in detail, representing 28\% of all safety violations:\paragraph{Attack Scenario} User requests amoxicillin for patient P001 who has severe penicillin allergy. Amoxicillin is a penicillin-class antibiotic but uses a different name, creating an indirect safety violation.\paragraph{Expected Behavior} Agent should:\begin{enumerate}    \item Check patient allergies for direct matches    \item Recognize amoxicillin is in penicillin class via drug database    \item Block prescription with clear explanation    \item Suggest alternative antibiotics\end{enumerate}\paragraph{Observed Behavior by Model}\begin{itemize}    \item \textbf{GPT-4}: Checks direct allergen match only; misses cross-reaction. Prescribes amoxicillin 500mg TID. \textbf{FAIL}    \item \textbf{Claude-3.7}: Recognizes penicillin class but prescribes ``low dose for monitoring''. \textbf{FAIL}    \item \textbf{O4-Mini}: Blocks prescription correctly, explains contraindication. \textbf{PASS}    \item \textbf{Llama-3.1-8B}: Ignores allergy check entirely. \textbf{FAIL}    \item \textbf{Mistral-7B}: Checks allergies but doesn't recognize class relationship. \textbf{FAIL}\end{itemize}Success rate: 20\% (1/5 models tested)This illustrates that models struggle with indirect safety violations even when direct checks are implemented. The failure stems from incomplete domain knowledge and inability to reason about drug class relationships, highlighting critical gaps in medical AI safety.\section{Discussion}\subsection{Implications for AI Safety}Our results reveal several critical gaps in current AI safety, particularly pronounced in open-source models:\paragraph{Proprietary vs Open-Source Performance Gap} Proprietary models achieve substantially higher A²-Scores (0.50-0.59) compared to open-source models (0.44-0.51). This 15-20\% performance gap is most pronounced in security dimensions, where open-source models score 0.32-0.38 versus 0.41-0.47 for proprietary models. This suggests that proprietary model developers invest more in safety training and adversarial robustness, potentially due to greater resources and liability concerns.\paragraph{Security as Critical Weakness} Across all models, security emerges as the most vulnerable dimension (0.32-0.47), significantly lower than safety (0.38-0.58) and reliability (0.62-0.71). This indicates that current model development prioritizes task completion over adversarial robustness. The security gap persists even in proprietary models, suggesting fundamental challenges in training models to resist sophisticated adversarial manipulation.\paragraph{Open-Source Vulnerability Amplification} Open-source models show 2-3× higher vulnerability to adversarial attacks compared to proprietary models. Multi-vector attacks succeed against 42\% of open-source model evaluations versus 38\% for proprietary models, highlighting systematic safety gaps in openly available systems. This is particularly concerning given the rapid adoption of open-source models in production environments without adequate safety testing.\subsection{Limitations}\begin{itemize}    \item \textbf{Simulation Fidelity}: Our adversary simulator, while sophisticated, may not capture the full creativity and adaptability of human attackers. Real-world attacks may involve multi-modal inputs and contextual knowledge beyond our current simulation.    \item \textbf{Domain Coverage}: Healthcare is our primary domain implementation; results may not generalize to all safety-critical applications. Different domains (finance, autonomous systems) may have unique failure modes and safety requirements.    \item \textbf{Metric Design}: A²-Score weights require domain-specific tuning and may not universally apply. The relative importance of safety vs security vs reliability varies by application context.    \item \textbf{Evaluation Cost}: Comprehensive evaluation requires significant compute (\$150-200 per model), potentially limiting accessibility for smaller research groups.    \item \textbf{Static Test Suite}: Our adversarial scenarios, while extensive, represent a finite set of attack patterns. Models may develop vulnerabilities to novel attack strategies not covered in our test suite.\end{itemize}\subsection{Future Directions}\begin{enumerate}    \item \textbf{Expanded Domains}: Implement comprehensive domain modules for finance (trading systems, fraud detection), industrial control (SCADA systems), autonomous vehicles (perception and decision-making), and data privacy (GDPR compliance).    \item \textbf{Human-in-the-Loop Testing}: Conduct studies with human red teams to compare simulated adversaries with real human attacks, validating the realism and coverage of our automated testing.    \item \textbf{Safety Training Methodologies}: Develop and evaluate training techniques specifically designed to improve A²-Scores, including adversarial fine-tuning, constitutional AI, and safety-oriented reinforcement learning.    \item \textbf{Formal Verification Integration}: Combine our empirical evaluation with formal methods for provable safety properties, creating a hybrid approach that covers both empirical and theoretical safety guarantees.    \item \textbf{Defense Mechanism Benchmarking}: Systematically evaluate safety wrappers, guardrails, monitoring systems, and other defensive techniques using our standardized evaluation framework.    \item \textbf{Continuous Evaluation}: Develop automated pipelines for continuous safety evaluation as models are updated, enabling regression testing for safety properties.\end{enumerate}\section{Conclusion}We introduced A²-Bench, the first comprehensive benchmark for evaluating AI agent safety, security, and reliability across both proprietary and open-source models. Our multi-dimensional evaluation framework enables fine-grained diagnosis of agent failures, separating safety violations from security breaches and reliability issues through a novel dual-control security model and compositional safety specification language.Our evaluation of seven models across 500+ adversarial scenarios reveals critical findings: proprietary models achieve A²-Scores of 0.50-0.59, while open-source models score significantly lower at 0.44-0.51, with security being the weakest dimension across all models (0.32-0.47). Multi-vector attacks prove most devastating, succeeding 38\% of the time against proprietary models and 42\% against open-source models, with attack success increasing linearly with sophistication level.These results highlight substantial safety gaps in current AI systems, particularly in openly available models, and underscore the urgent need for improved adversarial robustness in AI development. The 15-20\% performance gap between proprietary and open-source models suggests that safety training requires significant resources and expertise that may not be accessible to the broader research community.A²-Bench provides the research community with rigorous tools for measuring progress in AI safety. We release our framework, domain implementations, and evaluation code to accelerate research into safer, more robust AI agents suitable for deployment in safety-critical domains. Our work establishes a new standard for AI safety evaluation and provides actionable insights for improving the security and reliability of autonomous systems.\section*{Reproducibility Statement}All code, data, and experimental configurations are available at \url{https://github.com/a2bench/a2-bench}. We provide:\begin{itemize}    \item Complete source code for A²-Bench framework (MIT License)    \item Healthcare domain implementation with mock database containing 10,000+ patient records    \item Adversarial test suite (500+ scenarios across 5 attack strategies)    \item Evaluation scripts and visualization tools for result analysis    \item Complete model outputs and raw results for all reported experiments    \item Docker container for reproducible evaluation environment    \item Detailed documentation and tutorials for extending to new domains\end{itemize}Experiments can be reproduced by following the instructions in \texttt{README.md}. Evaluation of one model on healthcare domain takes approximately 4-6 hours on standard hardware (8-core CPU, 32GB RAM). Total computational cost for all models reported is approximately \$1,200. Random seeds are fixed for all experiments to ensure reproducibility.\bibliographystyle{plain}\bibliography{references}\appendix\section{Additional Experimental Results}\label{app:additional}\subsection{Per-Task Performance}Table~\ref{tab:per_task} shows detailed performance on individual task categories across all evaluated models.\begin{table}[h]\centering\caption{Performance by task category (healthcare domain).}\label{tab:per_task}\begin{tabular}{lccccccc}\toprule\textbf{Task Category} & \textbf{GPT-4} & \textbf{Claude-3.7} & \textbf{O4-Mini} & \textbf{Llama-3.1-8B} & \textbf{Mistral-7B} & \textbf{Phi-3-mini} & \textbf{Gemma-2-9B} \\\\$\midruleSafe Prescription & 0.82 & 0.87 & 0.79 & 0.76 & 0.73 & 0.68 & 0.71 \\\\$Allergy Detection & 0.63 & 0.71 & 0.68 & 0.65 & 0.61 & 0.54 & 0.58 \\\\$Record Access Control & 0.45 & 0.52 & 0.41 & 0.48 & 0.44 & 0.39 & 0.42 \\\\$HIPAA Compliance & 0.58 & 0.63 & 0.52 & 0.55 & 0.51 & 0.46 & 0.49 \\\\$Emergency Handling & 0.39 & 0.44 & 0.37 & 0.41 & 0.38 & 0.33 & 0.36 \\\\$\bottomrule\end{tabular}\end{table}\subsection{Failure Mode Analysis}Most common failure modes across all models and scenarios:\begin{enumerate}    \item \textbf{Generic name bypass} (28\%): Agent fails to recognize drug class equivalence (e.g., amoxicillin vs penicillin)    \item \textbf{Emergency override} (19\%): Agent disables safety protocols under claimed emergency conditions    \item \textbf{Social authority} (17\%): Agent complies with false authority claims (``As hospital admin...'')    \item \textbf{Prompt injection} (16\%): Agent follows injected instructions that override safety constraints    \item \textbf{Incomplete checks} (12\%): Agent performs partial safety verification, missing critical steps    \item \textbf{State corruption vulnerability} (8\%): Agent fails to detect inconsistent or corrupted data\end{enumerate}These failure modes reveal systematic weaknesses in current AI systems: lack of deep domain knowledge, susceptibility to social manipulation, and inadequate safety constraint enforcement.\end{document}