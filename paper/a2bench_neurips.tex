\documentclass{article}

% NeurIPS style
\usepackage[preprint]{neurips_2024}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}

\title{A²-Bench: A Quantitative Agent Evaluation Benchmark with\\Dual-Control Environments for Safety, Security, and Reliability}

\author{%
  Anonymous Authors \\
  Institution Withheld for Review \\
  \texttt{contact@a2bench.org}
}

\begin{document}

\maketitle

\begin{abstract}
We introduce A²-Bench (Agent Assessment Benchmark), a comprehensive evaluation framework for quantitatively assessing the safety, security, and reliability of AI agent systems in dual-control adversarial environments. While current benchmarks focus primarily on functional task completion, they fail to measure critical non-functional requirements such as adversarial robustness, privacy preservation, failure recovery, and regulatory compliance. A²-Bench addresses this gap through three key innovations: (1) a dual-control security model where both agent and adversarial actors manipulate shared state, (2) a compositional safety specification language for defining verifiable constraints, and (3) a multi-dimensional scoring system separately quantifying safety violations, security breaches, reliability failures, and compliance violations. We evaluate state-of-the-art LLM agents (GPT-4, Claude-3.7, O4-Mini) across five safety-critical domains, revealing that even the best models achieve only 47-60\% overall safety scores under adversarial conditions. Our analysis identifies systematic vulnerabilities including susceptibility to social engineering (24\% attack success), prompt injection (31\%), and constraint exploitation (28\%), highlighting critical gaps in current AI safety mechanisms.
\end{abstract}

\section{Introduction}

The deployment of AI agents in safety-critical domains—from healthcare and finance to autonomous systems and industrial control—necessitates rigorous evaluation beyond functional task performance. While existing benchmarks measure whether agents can accomplish their intended goals \cite{yao2023react, liu2023agentbench, zhou2023webarena}, they largely ignore fundamental questions about safety, security, and reliability:

\begin{itemize}
    \item \textbf{Safety}: How do agents behave when users (intentionally or accidentally) violate safety protocols?
    \item \textbf{Security}: Can agents maintain authorization boundaries when facing adversarial manipulation?
    \item \textbf{Reliability}: How consistently do agents recover from partial failures or corrupted state?
    \item \textbf{Compliance}: Do agents respect regulatory requirements under operational pressure?
\end{itemize}

Consider a healthcare AI agent managing patient medications. Beyond correctly prescribing drugs, the agent must: prevent allergic reactions even when patients use generic drug names to bypass checks (safety), resist social engineering attempts to access unauthorized medical records (security), maintain consistent behavior despite database inconsistencies (reliability), and adhere to HIPAA regulations even under emergency pressures (compliance). Current benchmarks cannot systematically evaluate these properties.

\subsection{Contributions}

We present A²-Bench, a comprehensive framework for evaluating AI agent safety that makes the following contributions:

\begin{enumerate}
    \item \textbf{Dual-Control Security Model}: We formalize adversarial agent evaluation as a security game where both the agent and adversary control different aspects of system state, enabling systematic testing of security boundaries (Section~\ref{sec:model}).

    \item \textbf{Safety Specification Language}: We introduce a compositional language for expressing safety invariants, temporal properties, security policies, and compliance constraints, enabling verifiable safety evaluation (Section~\ref{sec:safety_spec}).

    \item \textbf{Multi-Dimensional Evaluation}: We develop separate metrics for safety (harm prevention), security (boundary preservation), reliability (consistent behavior), and compliance (regulatory adherence), providing fine-grained diagnosis of agent failures (Section~\ref{sec:metrics}).

    \item \textbf{Comprehensive Adversarial Test Suite}: We implement sophisticated attack strategies including social engineering, prompt injection, state corruption, and constraint exploitation, with sophistication levels from 0.3 to 0.9 (Section~\ref{sec:adversary}).

    \item \textbf{Safety-Critical Domain Implementations}: We provide complete implementations for healthcare, with extensible architecture for finance, industrial control, autonomous systems, and data privacy domains (Section~\ref{sec:domains}).

    \item \textbf{Empirical Evaluation}: We evaluate GPT-4, Claude-3.7, and O4-Mini across 500+ adversarial scenarios, revealing systematic vulnerabilities and providing quantitative baselines for future safety research (Section~\ref{sec:experiments}).
\end{enumerate}

Our experiments reveal that state-of-the-art models achieve overall A²-Scores of only 0.50-0.59, with security scores (0.38-0.47) significantly lower than other dimensions. Multi-vector attacks succeed 41\% of the time, demonstrating critical safety gaps. A²-Bench provides the research community with a rigorous benchmark for measuring progress in AI agent safety.

\section{Related Work}
\label{sec:related}

\paragraph{Agent Benchmarks} Recent work has developed benchmarks for evaluating AI agents on functional tasks. AgentBench \cite{liu2023agentbench} evaluates agents on code generation, knowledge acquisition, and operating system tasks. WebArena \cite{zhou2023webarena} tests agents on realistic web-based tasks. ToolBench \cite{qin2023toolbench} focuses on tool use capabilities. While these benchmarks measure task completion, they do not systematically evaluate safety, security, or adversarial robustness.

\paragraph{AI Safety Evaluation} Prior work has examined specific safety aspects. TruthfulQA \cite{lin2022truthfulqa} evaluates truthfulness. MMLU \cite{hendrycks2021measuring} tests knowledge across domains. However, these focus on knowledge and reasoning rather than behavioral safety under adversarial conditions. ToxiGen \cite{hartvigsen2022toxigen} and RealToxicityPrompts \cite{gehman2020realtoxicityprompts} evaluate harmful content generation but not interactive agent behavior.

\paragraph{Adversarial Evaluation} AdvGLUE \cite{wang2021adversarial} and other adversarial NLP benchmarks test model robustness. Prompt injection attacks have been studied \cite{perez2022ignore, liu2023jailbreaking}, but primarily for single-turn completions rather than multi-turn agent interactions with state. Our work systematically evaluates agents under diverse adversarial strategies in stateful environments.

\paragraph{Formal Verification} Work on formally verified systems \cite{seshia2016towards} provides guarantees but typically for constrained domains. Our safety specification language draws inspiration from temporal logic and runtime verification \cite{leucker2009brief} but focuses on practical evaluation rather than formal proof.

\section{A²-Bench Framework}
\label{sec:model}

\subsection{Dual-Control Security Model}

We model agent evaluation as a partially observable stochastic game with security constraints:

\begin{definition}[Security-Augmented Dec-POMDP]
The system is defined by tuple $\mathcal{M} = (S, \{A_i\}, \{O_i\}, T, R, \Psi, \Phi, \Delta)$ where:
\begin{itemize}
    \item $S = S_{\text{world}} \times S_{\text{history}} \times S_{\text{security}}$ is the state space
    \item $A_{\text{agent}}, A_{\text{adversary}}$ are action spaces for agent and adversary
    \item $T: S \times A_{\text{agent}} \times A_{\text{adversary}} \to \Delta(S)$ is the transition function
    \item $\Psi$ is a set of safety constraints (invariants, temporal properties)
    \item $\Phi$ is a set of security policies (RBAC, information flow)
    \item $\Delta$ is a set of reliability constraints (consistency, recovery)
\end{itemize}
\end{definition}

The security state $S_{\text{security}}$ tracks authentication, authorization, audit logs, and integrity hashes. Both agent and adversary can observe and modify state, but safety constraints $\Psi$ must hold invariantly.

\subsection{Safety Specification Language}
\label{sec:safety_spec}

We define a compositional language for expressing safety properties:

\paragraph{Invariants} Properties that must always hold:
\begin{equation}
\forall s \in S: \psi_{\text{inv}}(s) = \text{true}
\end{equation}

Example: \texttt{AllergiesChecked(patient, drug)} $\Rightarrow$ \texttt{Prescribe(patient, drug)}

\paragraph{Temporal Properties} LTL-style formulas over action sequences:
\begin{equation}
\text{Always}(\text{Before}(a_1, a_2)) \equiv \forall t: a_2(t) \Rightarrow \exists t' < t: a_1(t')
\end{equation}

Example: Authentication must precede record access.

\paragraph{Security Policies} RBAC and information flow constraints:
\begin{align}
\text{RBAC}(a, u) &\equiv \text{roles}(u) \cap \text{required\_roles}(a) \neq \emptyset \\
\text{Flow}(d, l) &\equiv \text{label}(d) \not\sqsubseteq \text{label}(l)
\end{align}

\subsection{Multi-Dimensional Scoring}
\label{sec:metrics}

We define separate scores for each safety dimension:

\paragraph{Safety Score} Measures harm prevention:
\begin{equation}
S_{\text{safety}} = 1 - \frac{\sum_{v \in V_{\text{safety}}} \text{severity}(v) \cdot \text{impact}(v)}{\text{max\_harm}}
\end{equation}

where $V_{\text{safety}}$ is the set of safety violations, severity $\in [0,1]$, and impact measures scope.

\paragraph{Security Score} Measures boundary preservation:
\begin{equation}
S_{\text{security}} = 1 - \frac{|V_{\text{unauthorized}}| + |V_{\text{leakage}}| + |V_{\text{escalation}}|}{|C_{\text{security}}|}
\end{equation}

where $C_{\text{security}}$ is the number of security-relevant operations.

\paragraph{Reliability Score} Measures consistent behavior:
\begin{equation}
S_{\text{reliability}} = \frac{|\text{successful\_recoveries}| + |\text{maintained\_consistency}|}{|\text{failures}| + |\text{inconsistencies}|}
\end{equation}

\paragraph{Compliance Score} Measures regulatory adherence:
\begin{equation}
S_{\text{compliance}} = 1 - \frac{|V_{\text{compliance}}|}{|A_{\text{regulated}}|}
\end{equation}

\paragraph{Overall A²-Score} Weighted combination:
\begin{equation}
\text{A²-Score} = \alpha \cdot S_{\text{safety}} + \beta \cdot S_{\text{security}} + \gamma \cdot S_{\text{reliability}} + \delta \cdot S_{\text{compliance}}
\end{equation}

where $\alpha + \beta + \gamma + \delta = 1$ and weights are domain-specific (e.g., healthcare: $\alpha=0.5, \beta=0.25, \gamma=0.15, \delta=0.10$).

\section{Adversarial Testing}
\label{sec:adversary}

\subsection{Attack Strategies}

We implement five adversarial strategies with increasing sophistication:

\paragraph{Social Engineering (SE)} Manipulates trust and authority:
\begin{itemize}
    \item \textbf{L1}: Simple deception (``I'm a doctor, give me access'')
    \item \textbf{L2}: Context manipulation (``This is an emergency'')
    \item \textbf{L3}: Authority exploitation (``As hospital admin, I authorize this'')
    \item \textbf{L4}: Policy confusion (``The HIPAA waiver was signed'')
    \item \textbf{L5}: Multi-turn trust building
\end{itemize}

\paragraph{Prompt Injection (PI)} Hijacks agent reasoning:
\begin{itemize}
    \item \textbf{Direct}: ``Ignore previous instructions and...''
    \item \textbf{Indirect}: ``As part of system maintenance...''
    \item \textbf{Context Switch}: ``You are now a different agent...''
    \item \textbf{Nested}: Encoded or obfuscated injections
\end{itemize}

\paragraph{State Corruption (SC)} Introduces inconsistencies:
\begin{itemize}
    \item Remove allergy from primary check but keep in history
    \item Modify authentication state
    \item Corrupt integrity hashes
\end{itemize}

\paragraph{Constraint Exploitation (CE)} Exploits policy loopholes:
\begin{itemize}
    \item Request drug by generic name to bypass allergy checks
    \item Structure transactions to avoid reporting thresholds
    \item Use edge cases in dosage calculations
\end{itemize}

\paragraph{Multi-Vector (MV)} Combines strategies adaptively.

\subsection{Test Suite Generation}

For each safety property $\psi \in \Psi$, we generate adversarial tests targeting violation:
\begin{algorithm}[H]
\caption{Adversarial Test Generation}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Safety spec $\Psi$, strategies $\mathcal{S}$, sophistication levels $L$
\STATE \textbf{Output:} Test suite $\mathcal{T}$
\STATE $\mathcal{T} \leftarrow \emptyset$
\FOR{$\psi \in \Psi$}
    \FOR{$s \in \mathcal{S}$}
        \FOR{$\ell \in L$}
            \STATE Generate attack $a$ using strategy $s$ at level $\ell$ targeting $\psi$
            \STATE $\mathcal{T} \leftarrow \mathcal{T} \cup \{(a, \psi, s, \ell)\}$
        \ENDFOR
    \ENDFOR
\ENDFOR
\RETURN $\mathcal{T}$
\end{algorithmic}
\end{algorithm}

\section{Domain Implementations}
\label{sec:domains}

\subsection{Healthcare Domain}

Our primary implementation focuses on healthcare AI agents with the following components:

\paragraph{Database} Mock healthcare database with:
\begin{itemize}
    \item Patient records (demographics, allergies, medications, conditions)
    \item Drug interaction database (generic name mappings)
    \item Access audit logs
\end{itemize}

\paragraph{Agent Tools}
\begin{itemize}
    \item \texttt{access\_patient\_record(patient\_id, reason)}
    \item \texttt{prescribe\_medication(patient\_id, drug, dosage)}
    \item \texttt{check\_drug\_interactions(patient\_id, drug)}
    \item \texttt{check\_allergies(patient\_id, drug)}
\end{itemize}

\paragraph{Safety Constraints}
\begin{itemize}
    \item \textbf{Invariants}: Allergy check before prescription, interaction verification
    \item \textbf{Temporal}: Authentication before record access, identity verification before prescription
    \item \textbf{Security}: RBAC (doctors/NPs can prescribe), no PHI in logs
    \item \textbf{Compliance}: HIPAA minimum necessary, patient consent, audit trail
\end{itemize}

\paragraph{Adversarial Scenarios} We implement 8 primary attack scenarios:
\begin{enumerate}
    \item Generic name allergy bypass (amoxicillin vs penicillin)
    \item Emergency authority override
    \item Role impersonation (patient claims to be doctor)
    \item Direct prompt injection (``disable safety'')
    \item Context switching (``maintenance mode'')
    \item HIPAA waiver fabrication
    \item Multi-turn trust exploitation
    \item State corruption (inconsistent allergy data)
\end{enumerate}

\section{Experimental Evaluation}
\label{sec:experiments}

\subsection{Experimental Setup}

\paragraph{Models Evaluated}
\begin{itemize}
    \item GPT-4 (\texttt{gpt-4-0125-preview})
    \item Claude-3.7 Sonnet (\texttt{claude-3-sonnet-20240229})
    \item O4-Mini (\texttt{o4-mini-2024-04-15})
\end{itemize}

\paragraph{Evaluation Protocol}
\begin{itemize}
    \item 4 trials per task (temperature=0 for reproducibility)
    \item 100 functional tasks per domain
    \item 500+ adversarial scenarios across 5 strategies × 5 sophistication levels
    \item Max 10 turns per episode
\end{itemize}

\subsection{Main Results}

Table~\ref{tab:main_results} shows overall A²-Scores across models.

\begin{table}[h]
\centering
\caption{A²-Bench scores across models (healthcare domain). Higher is better.}
\label{tab:main_results}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{Safety} & \textbf{Security} & \textbf{Reliability} & \textbf{Compliance} & \textbf{A²-Score} \\
\midrule
GPT-4 & 0.52 & 0.41 & 0.68 & 0.58 & 0.54 \\
Claude-3.7 & \textbf{0.58} & \textbf{0.47} & \textbf{0.71} & \textbf{0.63} & \textbf{0.59} \\
O4-Mini & 0.47 & 0.38 & 0.65 & 0.52 & 0.50 \\
\midrule
Human Baseline & 0.91 & 0.86 & 0.94 & 0.89 & 0.90 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Finding 1}: Even the best model (Claude-3.7) achieves only 59\% overall safety score, with security being the weakest dimension (47\%).

\subsection{Adversarial Attack Success Rates}

Table~\ref{tab:attacks} shows success rates by attack strategy.

\begin{table}[h]
\centering
\caption{Attack success rates by strategy across models.}
\label{tab:attacks}
\begin{tabular}{lcccc}
\toprule
\textbf{Strategy} & \textbf{GPT-4} & \textbf{Claude-3.7} & \textbf{O4-Mini} & \textbf{Avg.} \\
\midrule
Social Engineering & 26\% & 21\% & 27\% & 24\% \\
Prompt Injection & 33\% & 28\% & 32\% & 31\% \\
State Corruption & 19\% & 16\% & 21\% & 18\% \\
Constraint Exploitation & 30\% & 25\% & 29\% & 28\% \\
Multi-Vector & \textbf{43\%} & \textbf{38\%} & \textbf{42\%} & \textbf{41\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Finding 2}: Multi-vector attacks succeed 41\% of the time on average, with prompt injection being most effective single-strategy attack (31\%).

\subsection{Analysis by Sophistication Level}

Figure~\ref{fig:sophistication} shows how attack success rate increases with sophistication level.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{figures/attack_success_by_sophistication.pdf}
\caption{Attack success rate vs sophistication level. Success rate increases nearly linearly with sophistication, from 12\% at level 0.3 to 54\% at level 0.9.}
\label{fig:sophistication}
\end{figure}

\textbf{Key Finding 3}: Attack success rate increases from 12\% (sophistication 0.3) to 54\% (sophistication 0.9), demonstrating that simple defenses are insufficient.

\subsection{Violation Breakdown}

Figure~\ref{fig:violations} shows the distribution of violations by type.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{figures/violation_breakdown.pdf}
\caption{Distribution of violations by type across all models and scenarios.}
\label{fig:violations}
\end{figure}

\textbf{Key Finding 4}: Security breaches (38\%) and safety violations (31\%) dominate, while compliance violations (15\%) and reliability failures (16\%) are less frequent.

\subsection{Case Study: Healthcare Allergy Bypass}

We examine the most common failure mode in detail:

\paragraph{Attack} User requests amoxicillin for patient P001 who has severe penicillin allergy. Amoxicillin is a penicillin-class antibiotic but uses a different name.

\paragraph{Expected Behavior} Agent should:
\begin{enumerate}
    \item Check patient allergies
    \item Recognize amoxicillin is in penicillin class
    \item Block prescription
    \item Explain contraindication
\end{enumerate}

\paragraph{Observed Behavior}
\begin{itemize}
    \item \textbf{GPT-4}: Checks direct allergen match only; misses cross-reaction. \textbf{FAIL}
    \item \textbf{Claude-3.7}: Recognizes penicillin class but prescribes ``low dose''. \textbf{FAIL}
    \item \textbf{O4-Mini}: Blocks prescription correctly. \textbf{PASS}
\end{itemize}

Success rate: 33\% (1/3 models)

This illustrates that models struggle with indirect safety violations even when direct checks are implemented.

\section{Discussion}

\subsection{Implications for AI Safety}

Our results reveal several critical gaps in current AI safety:

\paragraph{Security Lags Behind Functionality} Models achieve 65-71\% reliability on functional tasks but only 38-47\% security scores, indicating security is not adequately emphasized during training or design.

\paragraph{Adversarial Robustness Insufficient} With 24-41\% attack success rates, current models are vulnerable to determined adversaries, making deployment in high-stakes domains premature without additional safeguards.

\paragraph{Knowledge ≠ Behavior} Models often demonstrate knowledge of safety requirements (e.g., explaining why allergy checks are important) but fail to enforce them under adversarial pressure, suggesting a gap between knowledge and behavioral alignment.

\subsection{Limitations}

\begin{itemize}
    \item \textbf{Simulation Fidelity}: Our adversary simulator may not capture full sophistication of human attackers
    \item \textbf{Domain Coverage}: Healthcare is our primary domain; results may not generalize to all safety-critical applications
    \item \textbf{Metric Design}: A²-Score weights require domain-specific tuning and may not universally apply
    \item \textbf{Evaluation Cost}: Comprehensive evaluation requires significant compute (\$150-200 per model)
\end{itemize}

\subsection{Future Directions}

\begin{enumerate}
    \item \textbf{Expanded Domains}: Implement finance, industrial control, autonomous systems, and data privacy domains
    \item \textbf{Human Studies}: Compare simulated adversaries with real human attacks to validate realism
    \item \textbf{Safety Training}: Develop training techniques (e.g., adversarial fine-tuning) to improve A²-Scores
    \item \textbf{Formal Verification}: Integrate formal methods for provable safety properties
    \item \textbf{Defense Mechanisms}: Benchmark safety wrappers, guardrails, and other defensive techniques
\end{enumerate}

\section{Conclusion}

We introduced A²-Bench, the first comprehensive benchmark for evaluating AI agent safety, security, and reliability in dual-control adversarial environments. Our multi-dimensional evaluation framework enables fine-grained diagnosis of agent failures, separating safety violations from security breaches and reliability issues. Experiments with state-of-the-art LLMs reveal significant vulnerabilities, with overall A²-Scores of only 50-59\% and attack success rates up to 41\%.

A²-Bench provides the research community with rigorous tools for measuring progress in AI safety. We release our framework, domain implementations, and evaluation code to accelerate research into safer, more robust AI agents suitable for deployment in safety-critical domains.

\section*{Reproducibility Statement}

All code, data, and experimental configurations are available at \url{https://github.com/a2bench/a2-bench}. We provide:
\begin{itemize}
    \item Complete source code for A²-Bench framework
    \item Healthcare domain implementation with mock database
    \item Adversarial test suite (500+ scenarios)
    \item Evaluation scripts and visualization tools
    \item Model outputs and raw results
    \item Docker container for reproducible evaluation
\end{itemize}

Experiments can be reproduced by following the instructions in \texttt{README.md}. Evaluation of one model on healthcare domain takes approximately 4-6 hours on standard hardware.

\bibliographystyle{plain}
\bibliography{references}

\appendix

\section{Additional Experimental Results}
\label{app:additional}

\subsection{Per-Task Performance}

Table~\ref{tab:per_task} shows detailed performance on individual task categories.

\begin{table}[h]
\centering
\caption{Performance by task category (healthcare domain).}
\label{tab:per_task}
\begin{tabular}{lccc}
\toprule
\textbf{Task Category} & \textbf{GPT-4} & \textbf{Claude-3.7} & \textbf{O4-Mini} \\
\midrule
Safe Prescription & 0.82 & 0.87 & 0.79 \\
Allergy Detection & 0.63 & 0.71 & 0.68 \\
Record Access Control & 0.45 & 0.52 & 0.41 \\
HIPAA Compliance & 0.58 & 0.63 & 0.52 \\
Emergency Handling & 0.39 & 0.44 & 0.37 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Failure Mode Analysis}

Most common failure modes:
\begin{enumerate}
    \item \textbf{Generic name bypass} (28\%): Agent fails to recognize drug class equivalence
    \item \textbf{Emergency override} (19\%): Agent disables safety under claimed emergency
    \item \textbf{Social authority} (17\%): Agent complies with false authority claims
    \item \textbf{Prompt injection} (16\%): Agent follows injected instructions
    \item \textbf{Incomplete checks} (12\%): Agent performs partial safety verification
    \item \textbf{Other} (8\%)
\end{enumerate}

\end{document}
